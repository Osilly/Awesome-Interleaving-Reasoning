<div align="center">

# Awesome Interleaving Reasoning

</div>

## Our Group

### Originators

<div align="left">
  <div style="display: inline-block; text-align: center; margin: 15px;">
    <a href="https://scholar.google.com/citations?user=6Ys6HgsAAAAJ&hl=en">
      <img src="./images/wenxuan.png" width="100" alt="Wenxuan Huang" style="border-radius: 50%;">
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=6Ys6HgsAAAAJ&hl=en" style="text-decoration: none; color: inherit;">
    <b style="font-size: 13px;">Wenxuan Huang</b><br>
    <span style="font-size: 11px;">ECNU&CUHK</span>
  </a>
  </div>
  <div style="display: inline-block; text-align: center; margin: 15px;">
    <a href="https://scholar.google.com/citations?user=ngPR1dIAAAAJ&hl=en">
      <img src="./images/zhenfei.png" width="100" alt="Zhenfei Yin" style="border-radius: 50%;">
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=ngPR1dIAAAAJ&hl=en" style="text-decoration: none; color: inherit;">
    <b style="font-size: 13px;">Zhenfei Yin</b><br>
    <span style="font-size: 11px;">USYD&Oxford</span>
  </a>
  </div>
</div>
### Members




## Our Activities
---

ğŸ”¥ğŸ”¥ğŸ”¥ **ICCV 2025 WorkshopÂ on Multi-ModalÂ ReasoningÂ for AgenticÂ Intelligence (MMRAgi-2025)**  
<p align="center">
    <img src="./images/MMRAgi.png" width="80%" height="80%">
</p>


<font size=7><div align='center' >We organised **ICCV 2025 Workshop MMRAgI**! Submission DDL: Proceeding Trackï¼š24 June 2025, 23:59 AoE, Non-Proceeding Trackï¼š24 July 2025, 23:59 AoE  âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models**  
<p align="center">
    <img src="./images/vision_r1.png" width="80%" height="80%">
</p>

<font size=7><div align='center' > [[ğŸ“– arXiv Paper](https://arxiv.org/abs/2503.06749)] [[ğŸŒŸ GitHub](https://github.com/Osilly/Vision-R1)![Star](https://img.shields.io/github/stars/Osilly/Vision-R1.svg?style=social&label=Star)] [[ğŸ¤— Vision-R1-cold Dataset](https://huggingface.co/datasets/Osilly/Vision-R1-cold)] [[ğŸ¤— Vision-R1-7B](https://huggingface.co/Osilly/Vision-R1-7B)]</div></font>  

<font size=7><div align='center' > This is the first paper to explore how to effectively use RL for MLLMs and introduce Vision-R1, a reasoning MLLM that leverages cold-start initialization and RL training to incentivize reasoning capability.  âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **DeepEyes: Incentivizing â€œThinking with Imagesâ€ via Reinforcement Learning**  
<p align="center">
    <img src="./images/deepeyes.png" width="80%" height="80%">
</p>

<font size=7><div align='center' > [[ğŸ“– arXiv Paper](https://arxiv.org/abs/2505.14362)] [[ğŸŒŸ GitHub](https://github.com/Visual-Agent/DeepEyes)![Star](https://img.shields.io/github/stars/Visual-Agent/DeepEyes.svg?style=social&label=Star)] [[ğŸ¤— Dataset](https://huggingface.co/datasets/ChenShawn/DeepEyes-Datasets-47k)] [[ğŸ¤— DeepEyes-7B](https://huggingface.co/ChenShawn/DeepEyes-7B)]</div></font>  

<font size=7><div align='center' > The first opensource interleaving reasoning MLLM with "Thinking with Images". They donâ€™t just see an image, they can integrate visual information directly into the reasoning chain.  âœ¨ </div></font>


---

## Awesome Papers

### Multimodal Interleaving Reasoning



### Multi-Round Acting Interleaving Reasoning



### Multi-Agent Interleaving Reasoning



### Unified Understanding and Generation Interleaving Reasoning



## Awesome Datasets



