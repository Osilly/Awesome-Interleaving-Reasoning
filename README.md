<div align="center">

# Awesome Interleaving Reasoning

</div>

## Our Group

### Originators

<p align="left">
  &nbsp;&nbsp;
  <img src="./images/wenxuan_new.png" width="100" style="border-radius: 50%;">
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <img src="./images/zhenfei_new.png" width="100" style="border-radius: 50%;">
  <br>
  <a href="https://scholar.google.com/citations?user=6Ys6HgsAAAAJ&hl=en">
    <b>Wenxuan Huang</b>
  </a>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <a href="https://scholar.google.com/citations?user=ngPR1dIAAAAJ&hl=en">
    <b>Zhenfei Yin</b>
  </a>
  <br>
  &nbsp;&nbsp;
  ECNU&CUHK
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  USYD&Oxford
</p>


### Members




## Our Activities
---

ğŸ”¥ğŸ”¥ğŸ”¥ **ICCV 2025 WorkshopÂ on Multi-ModalÂ ReasoningÂ for AgenticÂ Intelligence (MMRAgi-2025)**  
<p align="center">
    <img src="./images/MMRAgi.png" width="50%" height="50%">
</p>


<font size=7><div align='center' >We organised **[ICCV 2025 Workshop MMRAgi](https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/MMR)**! <br> Submission DDL: Proceeding Trackï¼š24 June 2025, 23:59 AoE, Non-Proceeding Trackï¼š24 July 2025, 23:59 AoE  âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models**  
<p align="center">
    <img src="./images/vision_r1.png" width="80%" height="80%">
</p>

<font size=7><div align='center' > [[ğŸ“– arXiv Paper](https://arxiv.org/abs/2503.06749)] [[ğŸŒŸ GitHub](https://github.com/Osilly/Vision-R1)![Star](https://img.shields.io/github/stars/Osilly/Vision-R1.svg?style=social&label=Star)] [[ğŸ¤— Vision-R1-cold Dataset](https://huggingface.co/datasets/Osilly/Vision-R1-cold)] [[ğŸ¤— Vision-R1-7B](https://huggingface.co/Osilly/Vision-R1-7B)]</div></font>  

<font size=7><div align='center' > This is the first paper to explore how to effectively use RL for MLLMs and introduce Vision-R1, a reasoning MLLM that leverages cold-start initialization and RL training to incentivize reasoning capability.  âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **DeepEyes: Incentivizing â€œThinking with Imagesâ€ via Reinforcement Learning**  
<p align="center">
    <img src="./images/deepeyes.png" width="80%" height="80%">
</p>

<font size=7><div align='center' > [[ğŸ“– arXiv Paper](https://arxiv.org/abs/2505.14362)] [[ğŸŒŸ GitHub](https://github.com/Visual-Agent/DeepEyes)![Star](https://img.shields.io/github/stars/Visual-Agent/DeepEyes.svg?style=social&label=Star)] [[ğŸ¤— Dataset](https://huggingface.co/datasets/ChenShawn/DeepEyes-Datasets-47k)] [[ğŸ¤— DeepEyes-7B](https://huggingface.co/ChenShawn/DeepEyes-7B)]</div></font>  

<font size=7><div align='center' > The first opensource interleaving reasoning MLLM with "Thinking with Images". They donâ€™t just see an image, they can integrate visual information directly into the reasoning chain.  âœ¨ </div></font>


---

## Awesome Papers

### Multimodal Interleaving Reasoning



### Multi-Round Acting Interleaving Reasoning



### Multi-Agent Interleaving Reasoning



### Unified Understanding and Generation Interleaving Reasoning



## Awesome Datasets



